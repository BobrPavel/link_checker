import re
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urljoin

SUSPICIOUS_KEYWORDS = ["login", "secure", "verify", "update", "bank", "paypal", "signin", "account"]
TRACKING_PARAMS = ["utm_", "ref", "fbclid", "gclid", "mc_eid", "yclid", "igshid", "si"]

async def analyze_link(url: str) -> dict:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Å—ã–ª–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    """

    result = {
        "masked_domain": None,
        "is_punycode": False,
        "redirect_count": 0,
        "iframe_count": 0,
        "internal_links": 0,
        "external_links": 0,
        "tracking_params": [],
        "risk_flags": [],
    }

    parsed = urlparse(url)
    hostname = parsed.hostname or ""

    # üïµÔ∏è 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∫—É (–ø—Ä–∏–º–µ—Ä: google.com.hacker.io)
    parts = hostname.split(".")
    if len(parts) > 2:
        main_domain = ".".join(parts[-2:])
        subdomain = ".".join(parts[:-2])
        # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –ø–æ–¥–¥–æ–º–µ–Ω –ø–æ—Ö–æ–∂ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã
        if any(kw in subdomain.lower() for kw in ["google", "apple", "bank", "paypal", "amazon"]):
            result["masked_domain"] = f"‚ö†Ô∏è –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞: {subdomain}.{main_domain}"
            result["risk_flags"].append("masked_domain")

    # üß© 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Punycode
    if "xn--" in hostname:
        result["is_punycode"] = True
        result["risk_flags"].append("punycode")

    # üåê 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    try:
        timeout = aiohttp.ClientTimeout(total=10)
        headers = {"User-Agent": "Mozilla/5.0 (URLAnalyzerBot/1.0)"}
        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(url, allow_redirects=True, ssl=False) as response:
                result["redirect_count"] = len(response.history)
                html = await response.text(errors="ignore")

                soup = BeautifulSoup(html, "html.parser")

                # üîó 4. –ü–æ–¥—Å—á—ë—Ç —Å—Å—ã–ª–æ–∫
                for a in soup.find_all("a", href=True):
                    href = a["href"]
                    full = urljoin(url, href)
                    if full.startswith(url):
                        result["internal_links"] += 1
                    else:
                        result["external_links"] += 1

                # ü™ü 5. –ü–æ–¥—Å—á—ë—Ç iframe
                result["iframe_count"] = len(soup.find_all("iframe"))

                # üß≠ 6. –ü–æ–∏—Å–∫ —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                query_params = parse_qs(parsed.query)
                tracking = [k for k in query_params.keys() if any(tp in k for tp in TRACKING_PARAMS)]
                if tracking:
                    result["tracking_params"] = tracking
                    result["risk_flags"].append("tracking")

                # üß† 7. –ü–æ–∏—Å–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                body_text = soup.get_text(" ").lower()
                if any(kw in body_text for kw in SUSPICIOUS_KEYWORDS):
                    result["risk_flags"].append("phishing_keywords")

    except Exception as e:
        result["error"] = str(e)

    return result
